# -*- coding: utf-8 -*-
"""binary-classification-on-amazon-reviews-dataset.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1SVq3PRfcwTbH-zRRWvRlnjl3wXIyOju_

# Binary Classification of Amazon Food Review Dataset
"""

!pip list | grep "transformers"

import re
import string
from collections import Counter

import pandas as pd
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import Dataset, DataLoader
import nltk
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from scipy.stats import loguniform
from sklearn.pipeline import Pipeline
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.svm import SVC
from sklearn.metrics import classification_report
from sklearn.model_selection import GridSearchCV, train_test_split, RandomizedSearchCV
from sklearn.metrics import classification_report
from transformers import RobertaTokenizer, RobertaForSequenceClassification , Trainer, TrainingArguments

"""## Read the data"""

# Note: Can optionally use this if required
# df = pd.read_csv("hf://datasets/jhan21/amazon-food-reviews-dataset/Reviews.csv")

df = pd.read_csv("hf://datasets/jhan21/amazon-food-reviews-dataset/Reviews.csv")

df.shape

df.columns

df.Score.value_counts()

df = df[df['Score'] != 3]

df.shape

df.Score.value_counts()

"""## Convert the target variable to 0 and 1: 0 for Negative Reviews (1-2) and Positive Reviews (4-5)"""

df['target'] = df['Score'].apply(lambda x: 1 if x >= 4 else 0)
print(df['target'].value_counts())

"""## Handle Class Imbalance"""

# Split by class
positive = df[df['target'] == 1]
negative = df[df['target'] == 0]
positive_undersampled = positive.sample(n=len(negative), random_state=42)

# Combine and Shuffle
balanced_df = pd.concat([positive_undersampled, negative], axis=0)
balanced_df = balanced_df.sample(frac=1, random_state=42).reset_index(drop=True)
print(balanced_df['target'].value_counts())

"""## Preprocessing the Data"""

nltk.download('stopwords')
nltk.download('punkt_tab')

english_stopwords = set(stopwords.words('english'))
ps = PorterStemmer()

def preprocess_text(text):
    text = str(text)
    text = text.lower()
    text = re.sub(r'[^\w ]+','', text)
    text = re.sub(r'(http|https)?://\S+|www\.\S+','', text)
    text = ''.join(word for word in text if ord(word) < 128)
    text = text.translate(str.maketrans('','',string.punctuation))
    text = re.sub(r'[\d]+','', text)
    text = ' '.join(word for word in text.split() if len(word)>1)
    text = ' '.join(text.split())
    # stopword and punct removal
    text = ' '.join([i for i in nltk.word_tokenize(text) if i not in
    english_stopwords and i not in string.punctuation])
    # removal of anything other than English letters
    text = re.sub('[^a-z]+', ' ', text)
    text = ' '.join([ps.stem(i) for i in nltk.word_tokenize(text)]) #stemming
    return text

balanced_df['cleaned_text'] = balanced_df['Text'].apply(lambda x: preprocess_text(x))

# Optional: Save the dataset
balanced_df.to_csv("pre_processed_amazon_reviews.csv", index=False)

balanced_df['cleaned_text'].head()

"""## Do a Train Test Split: 70% Training, 15% Val and 15% Test"""

from sklearn.model_selection import train_test_split
train_df, temp_df = train_test_split(balanced_df, test_size=0.3, random_state=42)
val_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42)
print(f"Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}")

for x in [train_df, test_df, val_df]:
    print(x['target'].value_counts())

"""## Implement SVM"""



# SVM Pipeline
svm_pipeline = Pipeline([
    ('tfidf', TfidfVectorizer(max_features=10000)),
    ('clf', SVC(class_weight='balanced', kernel='rbf'))  # Default to RBF kernel
])

# Hyperparameter tuning
param_grid = {
    'clf__C': [0.1, 1, 10],
    'tfidf__ngram_range': [(1,1), (2,2)]
}

gs_svm = GridSearchCV(svm_pipeline, param_grid, cv=3, n_jobs=-1)
gs_svm.fit(train_df['cleaned_text'], train_df['target'])

# Evaluation
svm_preds = gs_svm.predict(val_df['cleaned_text'])
print(classification_report(val_df['target'], svm_preds))

"""## Implement LSTM


"""

# Create a custom Dataset class
class TextDataset(Dataset):
    def __init__(self, texts, labels, vocab, max_length):
        self.texts = texts
        self.labels = labels
        self.vocab = vocab
        self.max_length = max_length

    def __len__(self):
        return len(self.texts)

    def __getitem__(self, idx):
        text = self.texts[idx]
        label = self.labels[idx]
        numericalized = [self.vocab.get(token, self.vocab['<unk>']) for token in text.split()]
        padded = numericalized[:self.max_length] + [self.vocab['<pad>']]*(self.max_length - len(numericalized))
        return torch.tensor(padded), torch.tensor(label)

# Build vocabulary
def build_vocab(texts, max_vocab=20000):
    counter = Counter()
    for text in texts:
        counter.update(text.split())
    vocab = {'<pad>': 0, '<unk>': 1}
    for idx, (word, count) in enumerate(counter.most_common(max_vocab)):
        vocab[word] = idx + 2
    return vocab

# Load balanced dataset
texts = balanced_df['cleaned_text'].values
labels = balanced_df['target'].values

# Split data
X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Build vocabulary from training data
vocab = build_vocab(X_train)
vocab_size = len(vocab)
print(f"Vocabulary size: {vocab_size}")

# Create datasets
max_length = 128  # Sequence length
train_dataset = TextDataset(X_train, y_train, vocab, max_length)
val_dataset = TextDataset(X_val, y_val, vocab, max_length)
test_dataset = TextDataset(X_test, y_test, vocab, max_length)

# Create DataLoaders
batch_size = 64
train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)
val_loader = DataLoader(val_dataset, batch_size=batch_size)
test_loader = DataLoader(test_dataset, batch_size=batch_size)

# Define LSTM model
class LSTMClassifier(nn.Module):
    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim):
        super().__init__()
        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=0)
        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)
        self.fc = nn.Linear(hidden_dim, output_dim)
        self.dropout = nn.Dropout(0.3)

    def forward(self, text):
        embedded = self.dropout(self.embedding(text))
        output, (hidden, cell) = self.lstm(embedded)
        return self.fc(hidden.squeeze(0))

# Initialize model
device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
model = LSTMClassifier(
    vocab_size=vocab_size,
    embedding_dim=128,
    hidden_dim=256,
    output_dim=2  # Binary classification
).to(device)

# Handle class imbalance
class_counts = torch.bincount(torch.tensor(labels))
class_weights = 1. / class_counts.float()
criterion = nn.CrossEntropyLoss(weight=class_weights.to(device))
optimizer = optim.Adam(model.parameters(), lr=0.001)

# Training loop
def train(model, loader, criterion, optimizer):
    model.train()
    total_loss = 0
    for texts, labels in loader:
        texts, labels = texts.to(device), labels.to(device)
        optimizer.zero_grad()
        predictions = model(texts)
        loss = criterion(predictions, labels)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    return total_loss / len(loader)

# Evaluation function
def evaluate(model, loader, criterion):
    model.eval()
    total_loss = 0
    all_preds = []
    all_labels = []
    with torch.no_grad():
        for texts, labels in loader:
            texts, labels = texts.to(device), labels.to(device)
            predictions = model(texts)
            loss = criterion(predictions, labels)
            total_loss += loss.item()
            all_preds.extend(predictions.argmax(dim=1).cpu())
            all_labels.extend(labels.cpu())
    return total_loss / len(loader), all_preds, all_labels

# Train for 5 epochs
num_epochs = 5
for epoch in range(num_epochs):
    train_loss = train(model, train_loader, criterion, optimizer)
    val_loss, val_preds, val_labels = evaluate(model, val_loader, criterion)
    print(f'Epoch {epoch+1}:')
    print(f'\tTrain Loss: {train_loss:.3f} | Val Loss: {val_loss:.3f}')
    print(classification_report(val_labels, val_preds))

# Final test evaluation
_, test_preds, test_labels = evaluate(model, test_loader, criterion)
print("Test Performance:")
print(classification_report(test_labels, test_preds))

"""## Finetuning a RoBERTa Model"""

balanced_df.head()

print("GPU available:", torch.cuda.is_available())

device="cuda"

!pip install datasets

from transformers import RobertaTokenizer, RobertaForSequenceClassification, Trainer, TrainingArguments
from datasets import Dataset
from sklearn.model_selection import train_test_split
import pandas as pd
import numpy as np
from sklearn.metrics import accuracy_score, f1_score

# Load balanced dataset
texts = balanced_df['cleaned_text'].values
labels = balanced_df['target'].values

# Split data
X_train, X_temp, y_train, y_temp = train_test_split(texts, labels, test_size=0.3, random_state=42)
X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)

# Convert to DataFrames
train_df = pd.DataFrame({'text': X_train, 'label': y_train})
val_df = pd.DataFrame({'text': X_val, 'label': y_val})
test_df = pd.DataFrame({'text': X_test, 'label': y_test})

# Tokenization
tokenizer = RobertaTokenizer.from_pretrained('roberta-base')

def tokenize_function(x):
    return tokenizer(
        x['text'],
        padding='max_length',
        truncation=True,
        max_length=128
    )

# Prepare datasets
train_dataset = Dataset.from_pandas(train_df).map(tokenize_function, batched=True, num_proc=4)
val_dataset = Dataset.from_pandas(val_df).map(tokenize_function, batched=True, num_proc=4)
test_dataset = Dataset.from_pandas(test_df).map(tokenize_function, batched=True, num_proc=4)

# Define metric
def compute_metrics(pred):
    labels = pred.label_ids
    preds = pred.predictions.argmax(-1)
    f1 = f1_score(labels, preds, average="binary")
    acc = accuracy_score(labels, preds)
    return {"accuracy": acc, "f1": f1}

# Model Fine-Tuning
model = RobertaForSequenceClassification.from_pretrained(
    'roberta-base',
    num_labels=2
)
model = model.to(device)

# Training arguments
training_args = TrainingArguments(
    output_dir='/results',
    num_train_epochs=4,          # Adjusted epochs
    per_device_train_batch_size=64, # Adjusted batch size for memory
    per_device_eval_batch_size=128,
    evaluation_strategy="steps",
    logging_steps=100,
    fp16=True,
    gradient_accumulation_steps=4,
    save_strategy="steps",
    save_steps = 1000,
    learning_rate=5e-5,      # Fine-tuned learning rate
    weight_decay=0.01,
    load_best_model_at_end=True, # Load the best model based on val_loss
    metric_for_best_model="f1", # Use F1 score as the metric to select the best model
    greater_is_better=True,
    save_total_limit=1, # Save only the best model
    dataloader_num_workers=8 # Use multiple workers for data loading
)

from transformers import EarlyStoppingCallback

early_stopping = EarlyStoppingCallback(early_stopping_patience=3)

# Trainer setup
trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset,
    eval_dataset=val_dataset,
    compute_metrics=compute_metrics,
    callbacks = [early_stopping]
)

print("Started Training")

# Start training
trainer.train()

# Final Evaluation on Test Set
trainer.evaluate(test_dataset)

# Predictions on the test set
predictions = trainer.predict(test_dataset)
preds = np.argmax(predictions.predictions, axis=-1)
labels = predictions.label_ids

print("\nFinal Test Performance:")
print(classification_report(labels, preds))

# Final Evaluation on Test Set
trainer.evaluate(test_dataset)

# Predictions on the test set
predictions = trainer.predict(test_dataset)
preds = np.argmax(predictions.predictions, axis=-1)
labels = predictions.label_ids

print("\nFinal Test Performance:")
print(classification_report(labels, preds))

import matplotlib.pyplot as plt
from tensorflow.keras.preprocessing.sequence import pad_sequences

# Test set predictions
test_texts = test_df['reviewText'].tolist()
test_labels = test_df['sentiment'].values

# SVM
svm_preds = gs_svm.predict(test_df['clean_text'])

# LSTM
X_test = pad_sequences(tokenizer.texts_to_sequences(test_df['clean_text']), maxlen=200)
lstm_preds = (model.predict(X_test) > 0.5).astype(int)

# RoBERTa
test_encodings = tokenizer(test_texts, padding=True, truncation=True, max_length=512, return_tensors="pt")
roberta_preds = trainer.predict(test_encodings).predictions.argmax(axis=1)

# Metrics comparison
from sklearn.metrics import accuracy_score, f1_score

results = {
    'SVM': {
        'Accuracy': accuracy_score(test_labels, svm_preds),
        'F1': f1_score(test_labels, svm_preds)
    },
    'LSTM': {
        'Accuracy': accuracy_score(test_labels, lstm_preds),
        'F1': f1_score(test_labels, lstm_preds)
    },
    'RoBERTa': {
        'Accuracy': accuracy_score(test_labels, roberta_preds),
        'F1': f1_score(test_labels, roberta_preds)
    }
}

pd.DataFrame(results).T

# Training curves comparison
plt.figure(figsize=(12, 6))

# LSTM Training
plt.subplot(1, 2, 1)
plt.plot(history.history['accuracy'], label='LSTM Train')
plt.plot(history.history['val_accuracy'], label='LSTM Val')
plt.title('LSTM Training Progress')
plt.ylabel('Accuracy')
plt.legend()

# RoBERTa Training
plt.subplot(1, 2, 2)
roberta_history = trainer.state.log_history
train_loss = [x['loss'] for x in roberta_history if 'loss' in x]
val_loss = [x['eval_loss'] for x in roberta_history if 'eval_loss' in x]
plt.plot(train_loss, label='RoBERTa Train')
plt.plot(val_loss, label='RoBERTa Val')
plt.title('RoBERTa Training Progress')
plt.ylabel('Loss')
plt.legend()

plt.tight_layout()
plt.show()